{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Parallel Architecture Integrating FFN, 1D CNN, and LSTM for Predicting Wildfire Occurrences in Morocco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "from colorama import Fore, Style, init\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morocco Wildfire Predictions: 2010-2022 ML Dataset\n",
    "\n",
    "Download the dataset from: https://www.kaggle.com/datasets/ayoubjadouli/morocco-wildfire-predictions-2010-2022-ml-dataset\n",
    "\n",
    "Cite:\n",
    "\n",
    "Ayoub Jadouli, and Chaker El Amrani. (2024). Morocco Wildfire Predictions: 2010-2022 ML Dataset [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/8040722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize colorama\n",
    "init(autoreset=True)\n",
    "\n",
    "# Load the dataset\n",
    "final_dataset_balanced = pd.read_parquet('../Data/Data/FinalDataSet/Date_final_dataset_balanced_float32.parquet')\n",
    "\n",
    "# Split data into training and validation sets\n",
    "wf_df_train = final_dataset_balanced[final_dataset_balanced.acq_date < '2022-01-01']\n",
    "wf_df_valid = final_dataset_balanced[final_dataset_balanced.acq_date >= '2022-01-01']\n",
    "\n",
    "# Balance the dataset by taking the same number of positive and negative samples\n",
    "min_samples = min(wf_df_train['is_fire'].value_counts())\n",
    "wf_df_train_balanced = wf_df_train.groupby('is_fire').apply(lambda x: x.sample(min_samples)).reset_index(drop=True)\n",
    "\n",
    "min_samples_valid = min(wf_df_valid['is_fire'].value_counts())\n",
    "wf_df_valid_balanced = wf_df_valid.groupby('is_fire').apply(lambda x: x.sample(min_samples_valid)).reset_index(drop=True)\n",
    "\n",
    "wf_df_train_balanced = wf_df_train_balanced.sample(frac=1)\n",
    "wf_df_valid_balanced = wf_df_valid_balanced.sample(frac=1)\n",
    "\n",
    "# Removing dates\n",
    "acq_date_train = wf_df_train_balanced.pop('acq_date')\n",
    "acq_date_valid = wf_df_valid_balanced.pop('acq_date')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsets categorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to categorize lagged features based on column names\n",
    "def categorize_lagged_features_with_index_range(columns):\n",
    "    lagged_features = defaultdict(list)\n",
    "    lag_pattern = re.compile(r\"(.+)_lag_(\\d+)$\")\n",
    "    for idx, column in enumerate(columns):\n",
    "        match = lag_pattern.search(column)\n",
    "        if match:\n",
    "            feature_type = match.group(1)\n",
    "            lagged_features[feature_type].append(idx)\n",
    "    feature_ranges = {key: (min(values), max(values)) for key, values in lagged_features.items()}\n",
    "    return feature_ranges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model Implementation\n",
    "### Hybrid Parallel Architecture Integrating FFN, 1D CNN, and LSTM for Predicting Wildfire Occurrences in Morocco\n",
    "Tensorflow  Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D, GlobalAveragePooling1D, Concatenate, BatchNormalization, Dropout, Reshape\n",
    "\n",
    "def build_model(feature_ranges, total_features):\n",
    "    # Main input (batch_size, total_features)\n",
    "    main_input = Input(shape=(total_features,), name='main_input')\n",
    "    all_outputs = []\n",
    "\n",
    "    # FFN Block applied to the whole input\n",
    "    ffn_output = Dense(128, activation='relu')(main_input)\n",
    "    ffn_output = BatchNormalization()(ffn_output)\n",
    "    ffn_output = Dropout(0.3)(ffn_output)\n",
    "    ffn_output = Dense(32, activation='relu')(ffn_output)\n",
    "    ffn_output = BatchNormalization()(ffn_output)\n",
    "    ffn_output = Dropout(0.3)(ffn_output)\n",
    "    \n",
    "    #Combine outputs from FFN\n",
    "    all_outputs.append(ffn_output)\n",
    "\n",
    "    # Iterate through each feature type and setup their respective CNN and LSTM paths\n",
    "    for feature_type, (start_idx, end_idx) in feature_ranges.items():\n",
    "        num_features = end_idx - start_idx + 1\n",
    "\n",
    "        # Slicing input for each feature subset\n",
    "        subset_input = tf.slice(main_input, [0, start_idx], [-1, num_features])\n",
    "\n",
    "        # Reshape subset for 1D CNN (batch_size, num_features, 1)\n",
    "        cnn_input = Reshape((num_features, 1))(subset_input)\n",
    "        cnn_output = Conv1D(16, kernel_size=3, activation='relu', padding='same')(cnn_input)\n",
    "        cnn_output = GlobalAveragePooling1D()(cnn_output)\n",
    "\n",
    "        # Reshape input for LSTM to consider each feature as a timestep (batch_size, num_features, 1)\n",
    "        lstm_input = Reshape((num_features, 1))(subset_input)\n",
    "        lstm_output = LSTM(16)(lstm_input)\n",
    "\n",
    "        # Combine outputs from CNN and LSTM\n",
    "        combined_output = Concatenate()([cnn_output, lstm_output])\n",
    "        all_outputs.append(combined_output)\n",
    "\n",
    "    # Concatenate all outputs from different feature types including FFN output\n",
    "    final_concatenated = Concatenate()(all_outputs)\n",
    "\n",
    "    # Final dense layers\n",
    "    final_dense = Dense(64, activation='relu')(final_concatenated)\n",
    "    final_dense = BatchNormalization()(final_dense)\n",
    "    final_dense = Dropout(0.3)(final_dense)\n",
    "\n",
    "    final_dense = Dense(32, activation='relu')(final_dense)\n",
    "    final_dense = BatchNormalization()(final_dense)\n",
    "    final_dense = Dropout(0.3)(final_dense)\n",
    "    output_layer = Dense(1, activation='sigmoid')(final_dense)  # Assume binary classification\n",
    "\n",
    "    # Construct the model\n",
    "    model = Model(inputs=main_input, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the validation dataset into features and target\n",
    "X_train = wf_df_train_balanced.drop(columns=['is_fire'])\n",
    "y_train = wf_df_train_balanced['is_fire']\n",
    "\n",
    "# Split the validation dataset into features and target\n",
    "X_valid = wf_df_valid_balanced.drop(columns=['is_fire'])\n",
    "y_valid = wf_df_valid_balanced['is_fire']\n",
    "\n",
    "columns = wf_df_valid_balanced.columns\n",
    "feature_ranges=categorize_lagged_features_with_index_range(columns)\n",
    "\n",
    "# Initialize a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training features and transform both the training and validation features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Save the scaler for later use\n",
    "joblib.dump(scaler, \"../Models/scaler.joblib\")\n",
    "\n",
    "# Assume total features is the maximum index of your features plus one\n",
    "total_features = int(X_train.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the model with the feature ranges\n",
    "model = build_model(feature_ranges, total_features)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=20, batch_size=32, verbose=1, validation_data=(X_valid_scaled, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
